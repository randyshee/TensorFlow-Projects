# Rethinking Generalization
This project is inspired by "Understanding deep learning requires rethinking generalization" ([arXiv:1611.03530v2](https://arxiv.org/abs/1611.03530v2?utm_campaign=Nick%20Halstead&utm_medium=email&utm_source=Revue%20newsletter)) by Zhang et al. And codes are modification from "[Laboratory 2: Computer Vision](https://github.com/aamini/introtodeeplearning/tree/master/lab2)" of [MIT 6.S191: Introduction to Deep Learning.](http://introtodeeplearning.com)

In this project, I would be trying to reproduce some of the results from Zhang's paper by using MNIST dataset with the model being convolution neural network or fully connect neural network. They found that it's easy for a neural network to remember the train set so the accuracy of the test set would progress to zero with randomization of labels whereas the accuracy of the training set remains high.

Result from the notebook showed that for the models of convolution neural network (CNN) and fully connect neural network, both the accuracy of train and test set decreased as the randomness increased. However, if the number of epochs was set to an arbitrary large number, we would be able to see similar result to Zhang's. This is because with a large number of epochs, the network could memorize the whole data set (overfitting). Also, the reason why I did not increase the number of epochs for the CNN model was that the accuracy would not increase by a lot. This is because of the properties of CNN where the network was trying to find the local features of the images and would fail to map the features to the labels if the labels were set to be random. Note that the fully connect neural network actually memorize the randomized data set and that's why we should rethink about generalization in deep learning.

| *Accuracy of the Train and the Test Set at Different Randomization Level* |
|-------------------------|
| <img src="https://github.com/randyshee/TensorFlow-Projects/blob/main/Rethinking-Generalization/Image/2000%20Epochs.png"> |
